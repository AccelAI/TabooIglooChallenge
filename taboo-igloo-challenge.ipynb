{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\">\n",
    "<img src=\"https://static1.squarespace.com/static/58dc1a1ee4fcb51cbb80a096/t/58ebf4a0f7e0abb89e582c80/1521491055921/?format=1500w\">\n",
    "\n",
    "![No Guns](images/no-guns.png)\n",
    "\n",
    "<!-- Move the Accel AI logo to the right of the Ask logo, place the no guns image in the middle -->\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "# Taboo Igloo Challenge\n",
    "\n",
    "> As a website owner, you need to identify user queries about weapons so that you can protect your brand identity.\n",
    "\n",
    "In today's session we will:\n",
    "\n",
    "- Add textual features to real queries\n",
    "- Re-sample data as we have imbalanced classes\n",
    "- Train classification models\n",
    "- Compare and contrast model results\n",
    "\n",
    "**Disclaimer:** We are working with raw user text. We have attempted to remove offensive queries. However, we cannot guarentee that data are free of offensive material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We will use a several third-party Python in our analysis:\n",
    "\n",
    "- [**numpy**](https:http://www.numpy.org/): large array\n",
    "- [**pandas**](https://pandas.pydata.org/): data structures\n",
    "\n",
    "- [**scikit-learn**](http://scikit-learn.org/stable/index.html): data mining and analysis\n",
    "- [**imbalanced-learn**](http://contrib.scikit-learn.org/imbalanced-learn/stable/): support for working with imbalanced classes\n",
    "- [**spaCy**](https://spacy.io/): natural language processing\n",
    "\n",
    "- [**matplotlib**](https://matplotlib.org/): plotting library\n",
    "\n",
    "**Note:** We will need to load a word2vec model into spaCy. Since these are large files, spaCy distributes them separately from the core package. We will use the `en_vectors_web_lg` file, which you can install via:\n",
    "\n",
    "    python -m spacy download en_core_web_lg\n",
    "    \n",
    "You should consider running this notebook inside of a Python [virtual environment](https://docs.python.org/3/library/venv.html) if you are not running this notebook on [onepanel.io](https://www.onepanel.io/). Virtual environments mitigate the problems of managing system-wide packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import PCA\n",
    "import imblearn\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get extra space for making inline plots -- this lets us place plots side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Our data in a [JSON lines](http://jsonlines.org/) format. This format allows us to avoid some of the limitation of  comma separated value (CSV) files.\n",
    "\n",
    "    [\" Patty Hearst photos\", false]\n",
    "    [\"AK-47\", true]\n",
    "    \n",
    "We will use the Pandas [`read_json`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) method to read the file into a DataFrame. The argument `lines=True` instructs the parser to treat each line as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fcd5a17b5061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweapons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wepons-sample.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"weapon\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweapons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             obj = self._get_object_parser(\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             )\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 793\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "weapons = pd.read_json(\"weapons-sample.jsonl\", lines=True)\\\n",
    "            .rename(columns={0: \"query\", 1: \"weapon\"})\n",
    "weapons.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many observations we have in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapons.groupby('weapon').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our suspicion that we have an imblanced class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "We will transform the data in the following ways:\n",
    "\n",
    "1. Create textual features for each query. We will use a [word2vec](https://en.wikipedia.org/wiki/Word2vec) approach to create a [distributed representation](https://en.wikipedia.org/wiki/Artificial_neural_network#distributed_representation) for the queries. This transformation creates the features that we will use to predict whether a query is about weapons.\n",
    "\n",
    "2. Re-sample the data to account for having imblanced classes. Classification estimators do not work well when nearly all the data are in one class, as is our situation. This step balances the classes so that we will can estimate models.\n",
    "\n",
    "3. Split the data into train and test sets so that we can use cross-validation to check our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate textual features\n",
    "\n",
    "The idea of [word2vec](https://en.wikipedia.org/wiki/Word2vec) is that we can project words into a vector space. Word vectors with similar context and meaning are in close proximity. This transformation results in a vector for each query. We will use these vectors as the features in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the pipeline\n",
    "\n",
    "[Natural-language processing](https://en.wikipedia.org/wiki/Natural-language_processing) packages, such as [spaCy](https://spacy.io), begin with building a pipeline. The pipeline contains the algorithms and data that we will apply to our text. The function `spacy.load()` builds an NLP pipeline from a model file. The [`en_core_web_lg`](https://spacy.io/models/en#en_core_web_lg) model file contains data from a web crawl of news, blogs, and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune the vocabulary\n",
    "\n",
    "From our point of view, we don't need to distinguish between \"cat\" and \"feline\". The [`prune_vectors`](https://spacy.io/api/vocab#prune_vectors) method reduces the vocabularly, and hence the computational effort, without much impact on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = nlp.vocab.prune_vectors(500000)\n",
    "\n",
    "print(f\"Removed {len(removed) keys}.\")\n",
    "# let's see what we removed\n",
    "for key in (k for (i, k) in enumerate(removed.keys()) if i < 5):\n",
    "    print(f\"{key} -> {removed[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the feature matrix\n",
    "\n",
    "The Scikit-learn classifying models takes Numpy arrays as its inputs. Thus, we need an array with one row for each query and one column for each word vector element. We also need to convert the dependent variable in to array. The call to [`nlp.pipe`](https://spacy.io/api/pipe#pipe) allows spaCy to run through all of our queries in paralle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(weapons), nlp.vocab.vectors_length), dtype=np.float32)\n",
    "y = np.array(weapons['weapon'])\n",
    "\n",
    "# parse all the all the queries in parallel\n",
    "for (i, doc) in enumerate(nlp.pipe((str(x) for x in weapons['query']))):\n",
    "    # store each word vector as a row\n",
    "    X[i,] = doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results\n",
    "\n",
    "Let's take a look at this data so that we understand what we have. We will use [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) to reduce our vector space to two dimensions that we can graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_vis = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a helper function to capture the plotting logic. We will make several variations of the PCA scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splot(ax, X, y, title, labels=['Weapon', 'Clean'], alpha=0.3, samples=None,\n",
    "          xlim=[-5, 5], ylim=[-5, 5]):\n",
    "    if samples is not None and samples < len(X):\n",
    "        indx = np.random.choice(len(X), samples)\n",
    "        X = X[indx,]\n",
    "        y = y[indx]\n",
    "         \n",
    "    c_weapon = ax.scatter(X[y, 0], X[y, 1], label=labels[0], alpha=alpha)\n",
    "    c_clean = ax.scatter(X[y==False, 0], X[y==False, 1], label=labels[1], alpha=alpha)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # make the plot look pretty\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    ax.set_autoscaley_on(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return (c_weapon, c_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "splot(ax, X_vis, y, \"Original Sample\", samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing sets\n",
    "\n",
    "The last part of our ET process to group the data into a test and training sets. We split the data evenly since each set is equally import and we have enough observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the training set for the rest of the model development. We use the testing set once we are ready to validate our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-sample\n",
    "\n",
    "We want to resample our data so that we improve the balance between the minority and majority classes. One of the simpliest methods is to use a [naive random over-sampling](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html#random-over-sampler) technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results using PCA technique described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "X_train_vis = pca.fit_transform(X_train)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "cs = splot(ax1, X_vis, y, \"Original\", samples=1000)\n",
    "splot(ax2, X_train_vis, y_train, \"Random Over Sample\", samples=1000)\n",
    "\n",
    "plt.figlegend(cs, ('Weapon', 'Clean'), loc='lower center',\n",
    "              ncol=2, labelspacing=0.)\n",
    "plt.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we shouldn't expect these to have the same shape as we applied PCA to different matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your turn. Repeat this example with another re-sampling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers\n",
    "\n",
    "Our goal is a classification model that takes a user query and returns the probablity that the query refers to a weapon. We will use a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) as a benchmark. Logistic models two important properties for our application. Logistic modeles:\n",
    "\n",
    "- Treat the dependent variable (the query is about weapons) is a linear combination of all the features.\n",
    "- Require less computation than many other classification algorithms.\n",
    "\n",
    "These properties make logistic classifiers a suitable as a good benchmark when doing classification modeling. Scikit-learn defines a [`LogisticRegression`](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, y_train)\n",
    "predicted = logit.predict(X_test)\n",
    "print(sk.metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
