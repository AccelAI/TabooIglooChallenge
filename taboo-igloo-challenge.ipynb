{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\">\n",
    "<img src=\"https://static1.squarespace.com/static/58dc1a1ee4fcb51cbb80a096/t/58ebf4a0f7e0abb89e582c80/1521491055921/?format=1500w\">\n",
    "\n",
    "![No Guns](images/no-guns.png)\n",
    "\n",
    "<!-- Move the Accel AI logo to the right of the Ask logo, place the no guns image in the middle -->\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "# Taboo Igloo Challenge\n",
    "\n",
    "> As a website owner, you need to identify user queries about weapons so that you can protect your brand identity.\n",
    "\n",
    "In today's session we will:\n",
    "\n",
    "- Add textual features to real queries\n",
    "- Re-sample data as we have imbalanced classes\n",
    "- Train classification models\n",
    "- Compare and contrast model results\n",
    "\n",
    "**Disclaimer:** We are working with raw user text. We have attempted to remove offensive queries. However, we cannot guarentee that data are free of offensive material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Before we get started, we will need to install the notebook and third-party packages. We will also want to run this code inside a Python [virtual environment](https://docs.python.org/3/library/venv.html). Virtual environments mitigate the problems of managing system-wide packages.\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/AskMediaGroup/TabooIglooChallenge.git\n",
    "cd tabooigloochallege\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "jupyter notebook taboo-igloo-challenge.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We will use a several third-party Python packages in our analysis:\n",
    "\n",
    "- [**numpy**](https:http://www.numpy.org/): large array\n",
    "- [**pandas**](https://pandas.pydata.org/): data structures\n",
    "\n",
    "- [**scikit-learn**](http://scikit-learn.org/stable/index.html): data mining and analysis\n",
    "- [**imbalanced-learn**](http://contrib.scikit-learn.org/imbalanced-learn/stable/): support for working with imbalanced classes\n",
    "- [**spaCy**](https://spacy.io/): natural language processing\n",
    "\n",
    "- [**matplotlib**](https://matplotlib.org/): plotting library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import PCA\n",
    "import imblearn\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We will need to load a word embedding model into spaCy. Since these are large files, spaCy distributes them separately from the core package. We will use the `en_core_web_lg` file, which you can install via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get extra space for making inline plots -- this lets us place plots side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Our data in the [JSON lines](http://jsonlines.org/) format. This format allows us to avoid some of the limitation of  comma separated value (CSV) files.\n",
    "\n",
    "```json\n",
    "[\" Patty Hearst photos\", false]\n",
    "[\"AK-47\", true]\n",
    "[\"parsley, sage, rosemary, thyme\", false]\n",
    "```\n",
    "\n",
    "We will use the Pandas [`read_json`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) method to read the file into a DataFrame. The argument `lines=True` instructs the parser to treat each line as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapons = (pd.read_json(\"weapons-sample.jsonl\", lines=True) \n",
    "             .rename(columns={0: \"query\", 1: \"weapon\"}))\n",
    "weapons.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many observations we have in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapons.groupby('weapon').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our suspicion that we have an imblanced class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "We will transform the data in the following ways:\n",
    "\n",
    "1. Reduce the problem size through re-sampling. We have over a million records since there are relatively few cases of the minority class. We will re-sample the data to reduce the problem size and balance the classes.\n",
    "\n",
    "2. Create textual features for each query. We will use a [word2vec](https://en.wikipedia.org/wiki/Word2vec) approach to create a [distributed representation](https://en.wikipedia.org/wiki/Artificial_neural_network#distributed_representation) for the queries. This transformation creates the features that we will use to predict whether a query is about weapons.\n",
    "\n",
    "3. Split the data into train and test sets so that we can use cross-validation to check our predictions.\n",
    "\n",
    "4. Re-sample to balance the classes. Classifiers typically do work not well with severely imbalanced classes, such as our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the problem size\n",
    "\n",
    "Our first challenge is that computations on data of this size will take a long time. There are two feasible techniques to move forward:\n",
    "\n",
    "1. Move to a big data processing platform such as [Apache Spark](https://spark.apache.org).\n",
    "2. Down-sample the majority class.\n",
    "\n",
    "We will move forward with option 2 in this notebook and save the big processing solution for another time. Let's keep all the queries where `weapons` are `True` and take a 5% sample where `weapons` are `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = np.random.rand(len(weapons))\n",
    "weapons_sample = pd.concat((weapons[weapons.weapon],\n",
    "                            weapons[(weapons.weapon == False) & (np.random.rand(len(weapons)) < 0.05)]))\n",
    "print(weapons.weapon.value_counts())\n",
    "pd.concat((weapons_sample.head(), weapons_sample.tail()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate textual features\n",
    "\n",
    "The idea of [word2vec](https://en.wikipedia.org/wiki/Word2vec) is that we can project words into a vector space. Word vectors with similar context and meaning are in close proximity. This transformation results in a vector for each query. We will use these vectors as the features in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the pipeline\n",
    "\n",
    "[Natural-language processing](https://en.wikipedia.org/wiki/Natural-language_processing) packages, such as [spaCy](https://spacy.io), begin with building a pipeline. The pipeline contains the algorithms and data that we will apply to our text. The function `spacy.load()` builds an NLP pipeline from a model file. The [`en_core_web_lg`](https://spacy.io/models/en#en_core_web_lg) model file contains data from a web crawl of news, blogs, and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the feature matrix\n",
    "\n",
    "The Scikit-learn classifying models takes Numpy arrays as its inputs. Thus, we need an array with one row for each query and one column for each word vector element. We also need to convert the dependent variable in to array. The call to [`nlp.pipe`](https://spacy.io/api/pipe#pipe) allows spaCy to run through all of our queries in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(weapons_sample), nlp.vocab.vectors_length), dtype=np.float32)\n",
    "y = np.array(weapons_sample['weapon'])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# parse all the all the queries in parallel\n",
    "for (i, doc) in enumerate(nlp.pipe((str(x) for x in weapons_sample['query']))):\n",
    "    # store each word vector as a row\n",
    "    X[i,] = doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results\n",
    "\n",
    "Let's take a look at this data so that we understand what we have. We will use [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) to reduce our vector space to two dimensions that we can graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_vis = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a helper function to capture the plotting logic. We will make several variations of the PCA scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splot(ax, X, y, title, labels=['Weapon', 'Clean'], alpha=0.3, samples=None,\n",
    "          xlim=[-5, 5], ylim=[-5, 5]):\n",
    "    if samples is not None and samples < len(X):\n",
    "        indx = np.random.choice(len(X), samples)\n",
    "        X = X[indx,]\n",
    "        y = y[indx]\n",
    "         \n",
    "    c_weapon = ax.scatter(X[y, 0], X[y, 1], label=labels[0], alpha=alpha)\n",
    "    c_clean = ax.scatter(X[y==False, 0], X[y==False, 1], label=labels[1], alpha=alpha)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # make the plot look pretty\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    ax.set_autoscaley_on(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    return (c_weapon, c_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "splot(ax, X_vis, y, \"Original Sample\", samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing sets\n",
    "\n",
    "We now group the data into a test and training sets. We split the data evenly since each set is equally import and we have enough observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the training set for the rest of the model development. We use the testing set once we are ready to validate our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-sample\n",
    "\n",
    "We want to resample our data so that we improve the balance between the minority and majority classes. One of the simpliest methods is to use a [naive random over-sampling](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html#random-over-sampler) technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results using PCA technique described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "X_ros_vis = pca.fit_transform(X_ros)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "cs = splot(ax1, X_vis, y, \"Original\", samples=1000)\n",
    "splot(ax2, X_ros_vis, y_ros, \"Random Over Sample\", samples=1000)\n",
    "\n",
    "plt.figlegend(cs, ('Weapon', 'Clean'), loc='lower center',\n",
    "              ncol=2, labelspacing=0.)\n",
    "plt.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we shouldn't expect the same shape in both plots as we applied PCA to different matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your turn. Repeat this example with re-sampling strategy. Be sure to plot the results, following the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "[SMOTE](https://link.springer.com/content/pdf/10.1007%2F978-3-540-39804-2_12.pdf) is a technique for synthezing minority class obersvations. The algorithms uses [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) to new observations in the neighborhood of mintority class observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN\n",
    "\n",
    "[Adaptive Synthetic Sampling Approach (ADASYN)](http://www.ele.uri.edu/faculty/he/PDFfiles/adasyn.pdf), like SMOTE, ADASYN sythesizes minority class observations. The major difference is that ADASYN synthesizes difficult to learn observations. That is, minority class observations near observations in the majority class.\n",
    "\n",
    "Use the [ADASYN](http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.ADASYN.html) class in imbalanced-learn to re-sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype generation\n",
    "\n",
    "[Prototype generation](http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#cluster-centroids) in an *under-sampling* technique. Unlike SMOTE and ADASYN, prototype generation reduces the numbers of observations in the majority class. Prototype generation uses [K-means](https://en.wikipedia.org/wiki/K-means_clustering)) to synthesize observations to replace multiple direct observations.\n",
    "\n",
    "Use the [`ClusterCentroids`](http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.under_sampling.ClusterCentroids.html) imblanced-learn class to re-sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype selection\n",
    "\n",
    "[Protoype selection](http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#prototype-selection) under-samples the majority class. Unlike prototype generation, prototype selection does not synthesize observations. Instead, prototype selection relies on K-means to find clusters within the majority and then selects a represetative from observations in the clusters.\n",
    "\n",
    "There are several members in the prototype selection family. Let's random under sampler, as implemented in [`RandomUnderSampler`](http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#controlled-under-sampling-techniques) class as this is relatively straight forward to describe. Set `replacement=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflect\n",
    "\n",
    "Before moving forward, reflect on the results:\n",
    "\n",
    "1. What are the strengths and weaknesses of each approach?\n",
    "\n",
    "2. Which method will you use?\n",
    "\n",
    "3. List at least two reasons for being optimistic about estimating a model with these data.\n",
    "\n",
    "4. List at least two reasons for being pesimistic.\n",
    "\n",
    "Now, set `X_resampled` and `y_resampled` to the matrices that you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = X_ros, y_ros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers\n",
    "\n",
    "Our goal is a classification model that takes a user query and returns the probablity that the query refers to a weapon. We will use a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) as a benchmark. Logistic models two important properties for our application. Logistic models:\n",
    "\n",
    "- Treat the dependent variable (the query is about weapons) is a linear combination of all the features.\n",
    "- Require less computation than other common classification algorithms.\n",
    "\n",
    "These properties make logistic classifiers a suitable as a good benchmark when doing classification modeling. Scikit-learn defines a [`LogisticRegression`](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression()\n",
    "%time logit.fit(X_resampled, y_resampled)\n",
    "predicted_insample = logit.predict(X_train)\n",
    "print(sk.metrics.classification_report(y_train, predicted_insample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We commonly measure the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of a classifier to  assess performance.\n",
    "\n",
    "Let's use:\n",
    "\n",
    "- **T** for predictions that match observed data \n",
    "- **F** for predictions that do *not* match observed data\n",
    "- **P** for positive predictions (weapons)\n",
    "- **N** for negative predictions (no weapons)\n",
    "\n",
    "Precision is $\\frac{TP}{TP + FP}$.\n",
    "\n",
    "Recall is $\\frac{TP}{TP + FN}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time predicted = logit.predict(X_test)\n",
    "print(sk.metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your turn. Try fitting the data to each of the following models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "[Decision tree](https://en.wikipedia.org/wiki/Decision_tree) models that takes it name and form from [tree](https://en.wikipedia.org/wiki/Tree_%28data_structure%29). Each of the interior nodes represent a model feature. The exterior nodes represent the classification. This illustration, from Wikipdia, shows a descision tree trained on the Titantic survial data.\n",
    "\n",
    "![decision tree illustration from wikipedia](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
    "\n",
    "Use the [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/tree.html#classification) class in Scikit-learn to train the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Foreset\n",
    "\n",
    "The [random forest model](https://en.wikipedia.org/wiki/Random_forest) is an *ensemble* model. Ensemble models are composed of two or models. Random forests are composed of mutliple decision trees. Random forests have the advantage of avoiding some of the [overfitting](https://en.wikipedia.org/wiki/Overfitting) issues that are common in decision tree models.\n",
    "\n",
    "Use the [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class in Scikit-learn to train the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model\n",
    "\n",
    "Let's see well your favorite model on a set *loaded* keywords.\n",
    "\n",
    "Reset the variable `model` to point to your preferred model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a file of suspect text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test-words.txt\") as f:\n",
    "    suspect = [x.strip() for x in f.readlines()] # .strip() removes the new line character\n",
    "    \n",
    "suspect[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = np.vstack(x.vector for x in nlp.pipe(suspect))\n",
    "vectorized[:10, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model to the word vector matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time probs = model.predict_proba(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\"query\": suspect, \"prediction\": probs[:,1]})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Answer the following questions about your models:\n",
    "\n",
    "1. As a website owner, would you be comfortable with any of these models? Which model would you select?\n",
    "\n",
    "2. Describe how model you chose works.\n",
    "\n",
    "3. How much time does the model need to process one query?\n",
    "\n",
    "4. What are the strengths and limitations of your chose model compared to the logitic regression model? If you prefer the logistic regression model, what are its strengths and limitations compared to the next best alternative?\n",
    "\n",
    "5. What steps would you take to improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- What textual features did we add?\n",
    "\n",
    "- List three re-sampling techniques for imbalanced classes\n",
    "\n",
    "- What method do we call to train a classification model? What is the input?\n",
    "\n",
    "- What is one advantage of a logitistic model? What is one disadvantage?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
